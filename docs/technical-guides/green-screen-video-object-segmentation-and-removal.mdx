---
title: "How to build video effects like Green Screen using AI"
description: "A technical guide and example of how to build AI-based video effects features"
---

<Example>
  <div class="flex items-center justify-center">
    <img class="w-full" src="https://camo.githubusercontent.com/fcab9b95f7cb63fe497c9c645e1a7c001c5f42e2c7ebc0ec9367e3503fcb63dd/68747470733a2f2f696d6775722e636f6d2f707972594b434a2e676966" />
  </div>
</Example>

Recent research in Computer Vision (CV) has led to widespread advances in all sorts of use cases and creative media / video editing is no exception. Companies like Adobe are releasing features such as [motion tracking](https://www.adobe.com/products/aftereffects/motion-tracking.html) and [content-aware fills](https://helpx.adobe.com/after-effects/using/content-aware-fill.html) within Adobe After Effects while companies like RunwayML are releasing [green screen](https://runwayml.com/green-screen/) and [inpainting](https://runwayml.com/inpainting/). While it’s become evident that these features add immense value to users and could enhance a given product offering, the implementation of these features are only gated to a few companies that have large machine learning teams with the necessary resources to pull it off. The reality is that video infrastructure is hard — and when you pair that with AI / ML infrastructure, it becomes an extremely daunting task. As such, this post will go over using [Sieve](https://www.sievedata.com/) to build a complex video editing feature like video object removal / video green screen.

### Simplify the Problem

Building a modular green screen feature can mean many things. Let’s define a set of variables which can help us evaluate different approaches.

- **Cost.** Does it break the bank, especially when GPUs come int play?
- **Quality.** Are the masks high quality, and up to par with what the user expects?
- **Smoothness / Feel.** How smooth is the user experience?

#### Approach 1: MiVOS

One promising approach is Modular Interactive Video Object Segmentation ([MiVOS](https://hkchengrex.github.io/MiVOS/)). It’s from a CVPR paper released in late 2021.

<Example>
  <div class="flex items-center justify-center">
    <img class="w-full" src="https://camo.githubusercontent.com/cc1db1c9fe7a4cf5fe144330e6ba069db3be880c606a6626607a342629964847/68747470733a2f2f696d6775722e636f6d2f5131636b32544a2e676966" />
  </div>
</Example>

At first glance, this looks like everything we want. Users can click on any object and some of the out-of-the-box results seem to work at a high-quality. The issue comes up when we evaluate cost and smoothness. MiVOS is a large model that can’t be parallelized and is expensive to run, not to mention constant network calls. This means that a mask will have to be recalculated after each click (which takes a couple of seconds) and will require extremely expensive cloud hardware to run. This fails the cost and smoothness test.

#### Approach 2: PointRend

Another promising approach is to use a simpler model called [PointRend](https://github.com/facebookresearch/detectron2/tree/main/projects/PointRend). It was released in 2019 and boasts a more pixel-perfect result compared to other, more traditional instance segmentation models such as DeeplabV3, DeeplabV3+, etc.

<Example>
  <div class="flex items-center justify-center">
    <img class="w-full" src="https://github.com/ayoolaolafenwa/PixelLib/raw/master/Images/compare2.jpg" />
  </div>
</Example>

The interesting thing about this model is that it’s an image-based model. That means that it can be setup in parallel, where thousands of machines process individual frames at the same time -- rather than processing frames one after another. After the object masks are generated in parallel, we can run a tracking algorithm such as SORT or DeepSORT to associate bounding boxes to a given object. The way to think about this setup is almost like a MapReduce problem, where we “map” PointRend to every frame (by running it on thousands of machines in parallel) and then run an object tracking step at the end to reduce the results into given “objects” in a video.

### Shipping Green Screen with Sieve

Now we’ve gone over the approach to building a green screen effect using state of the art computer vision, but how does one set this up in practice? Typically, this is the hardest part of the problem because of all the cloud infrastructure, GPUs, and parallel processing involved (scaling to 1000s of parallel machines) but [Sieve’s platform](https://www.sievedata.com/) has made this extremely easy to do.

**Get an API Key**

Visit [Sieve dashboard](https://app.sievedata.com/) to sign up and click “Copy API Key”.

**Create a pre-configured green screen Project**

```Terminal bash
curl 'https://api.sievedata.com/v1/init_project' \
  -X POST \
  -H "Content-Type: application/json" \
  -H "X-API-Key: YOUR_API_KEY" \
  -d '{
    "project_name": "object_removal",
    "config_url": "https://storage.googleapis.com/sieve-preconfigured-projects/configs/segmentation.json"
  }'
```

**Push a Sample Video**

```Terminal bash
curl https://api.sievedata.com/v1/push_video \
  -X POST \
  -H "X-API-Key: YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "project_name": "object_removal",
    "video_url": "https://storage.googleapis.com/sieve-preconfigured-projects/videos/home-interior/home_sf.mp4",
    "video_name": "home_video"
  }'
```

**Poll Job Results**

```Terminal bash
curl 'https://api.sievedata.com/v1/get_all_jobs?project_name=object_removal' \
  -X GET \
  -H "X-API-Key: YOUR_API_KEY" \
  -H "Content-Type: application/json"
```

**Visualize Results**

Make queries to [Sieve](https://www.sievedata.com/) using the [metadata endpoint](https://docs.sievedata.com/query/metadata) to get object and mask information, and visualize all results.

<Example>
  <div class="flex items-center justify-center">
    <iframe class="h-full" src="https://storage.googleapis.com/sieve-data-video-test-bucket/dancing.webm" title="Dancing Object Masks"></iframe>
  </div>
</Example>


Large vision AI models like PointRend are increasingly being used for a variety of tasks. Inevitably, there are many other use cases in video editing that can be powered with AI. At [Sieve](https://www.sievedata.com/), we believe providing a simple experience to building video AI features can help companies easily enhance their products with minimal work required. To start using [Sieve](https://www.sievedata.com/) for any video AI use case, [get in touch with our team](https://docs.google.com/forms/d/e/1FAIpQLSdoXPC_FXbp_J-S7mp1-0MPIjXN1x84eY1Rqa6cUjvn-D_HRQ/viewform?usp=sf_link).